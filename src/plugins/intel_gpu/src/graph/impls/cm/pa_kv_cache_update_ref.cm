/*******************************************************************************
 * Copyright (c) 2022-2025 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

#include <cm/cm.h>
#include <cm/cmtl.h>

#ifndef ATTR
#define ATTR [[type("svmptr_t")]]
#define ATTR_BUF [[type("buffer_t")]]
#endif

constexpr uint wg_size = WG_SIZE;
#define REG_K 16

extern "C" _GENX_MAIN_ void KERNEL_NAME(
    const half* key [[type("svmptr_t")]],
    const half* value [[type("svmptr_t")]],
    const int32_t* past_lens [[type("svmptr_t")]],
    const int32_t* block_indices [[type("svmptr_t")]],
    const int32_t* block_indices_begins [[type("svmptr_t")]],
    const int32_t* subsequence_begins [[type("svmptr_t")]],
#if KV_CACHE_COMPRESSION_PER_TOKEN
    uint8_t* key_cache [[type("svmptr_t")]],
    uint8_t* value_cache [[type("svmptr_t")]],
#else
    half* key_cache [[type("svmptr_t")]],
    half* value_cache [[type("svmptr_t")]],
#endif   
    uint32_t key_pitch,
    uint32_t key_offset,
    uint32_t value_pitch,
    uint32_t value_offset,
    uint32_t batch_size_in_sequences) {
    // # key:   [batch_size_in_tokens, num_kv_heads * k_head_size]
    // # value  [batch_size_in_tokens, num_kv_heads * v_head_size]
    // # key_cache:   [num_blocks, num_heads, block_size, k_head_size]
    // # value_cache: [num_blocks, num_heads, block_size, v_head_size]
    // 
    // # past_lens: [sequences_num]
    // # subsequence_begins: [sequences_num + 1]
    // # block_indices: [used_blocks_num]
    // # block_indices_begins: [sequences_num + 1]

    // wg_count = aligned_to(batch_size_in_tokens, wg_size) // wg_size
    // # GWS [1, num_heads, wg_count * wg_size]
    // # LWS [1, 1, wg_size]

    const auto head_idx = cm_group_id(1);
    const auto wg_id = cm_group_id(2);
    const auto wg_local_id = cm_local_id(2);
    const auto local_size = cm_local_size(2);

    const uint token_idx = cm_global_id(2);

    // token_idx -> subsequence_idx
    if (token_idx >= subsequence_begins[batch_size_in_sequences]) return;
    uint subsequence_idx = 0;
    for (uint i = 0; i < batch_size_in_sequences; i++) {
        if (token_idx >= subsequence_begins[i] && token_idx < subsequence_begins[i + 1]) {
            subsequence_idx = i;
            break;
        }
    }

    const uint subsequence_begin_idx = subsequence_begins[subsequence_idx];
    const uint past_len = past_lens[subsequence_idx];
    const uint current_block_idx = (past_len + token_idx - subsequence_begin_idx) / PAGED_ATTENTION_BLOCK_SIZE;
    const uint token_start_pos = (past_len + token_idx - subsequence_begin_idx) % PAGED_ATTENTION_BLOCK_SIZE;
    const uint block_offset = block_indices_begins[subsequence_idx] + current_block_idx;

    #if KV_CACHE_COMPRESSION_PER_TOKEN
        // Per-token compression helper (legacy layout)
        auto quantize_and_store_token = [&](vector<half, K_HEAD_SIZE> data, uchar* out, uint out_offset, uint token_pos) {
            uint scale_offset = out_offset + K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE + token_pos * sizeof(half);
            half max_val = cm_reduced_max<half>(data);
            half min_val = cm_reduced_min<half>(data);
            half scale_val = half(0.0);
            half zp_val = half(0.0);
            if (max_val == min_val) {
                scale_val = half(0.0);
                zp_val = max_val;
            } else {
                scale_val = 255.0 / (max_val - min_val);
                zp_val = (0.0 - min_val) * scale_val;
            }
            vector<half, K_HEAD_SIZE>  dequant_data = cm_mul<half>(data, scale_val) + zp_val;
            vector<uchar, K_HEAD_SIZE> data_u8 = cm_rnde<uchar, K_HEAD_SIZE>(dequant_data);
            cm_ptr_store<uint32_t, K_HEAD_SIZE / 4>((uint32_t*)(out + out_offset + token_pos * K_HEAD_SIZE), 0, data_u8.format<uint32_t>());
            half *out_scale_zp = (half*)(out + scale_offset);
            out_scale_zp[0] = (max_val - min_val) / 255.0;
            out_scale_zp[PAGED_ATTENTION_BLOCK_SIZE] = zp_val;
        };

        // Per-channel compression helpers (new layout) activated when IS_KEY_BY_CHANNEL
        #ifdef IS_KEY_BY_CHANNEL
        auto quantize_and_store_channel_block_init = [&](const vector<half, K_HEAD_SIZE>& token_vec,
                                                         uchar* out,
                                                         uint block_base_offset) {
            // First token in block: initialise scale/zp per channel.
            // Layout: for channel c -> [block_base_offset + c*(PAGED_ATTENTION_BLOCK_SIZE+4)]
            // bytes: [quant_tokens(block_size)][scale(half)][zp(half)]
            #pragma unroll
            for (uint c = 0; c < K_HEAD_SIZE; ++c) {
                uint channel_offset = block_base_offset + c * (ADJUSTED_PAGED_ATTENTION_BLOCK_SIZE);
                half v = token_vec[c];
                // Use small epsilon to avoid zero range
                half max_val = v;
                half min_val = v;
                half range = max_val - min_val;
                if (range == half(0.0f)) range = half(1.0f / 255.0f);
                half scale_val = half(255.0f) / range;
                half zp_val = (half(0.0f) - min_val) * scale_val;
                // quantize single token
                half q_half = v * scale_val + zp_val;
                int q_int = (int)cm_rnde_sat<uchar>(q_half);
                out[channel_offset + 0] = (uchar)q_int;
                half* comp_ptr = (half*)(out + channel_offset + PAGED_ATTENTION_BLOCK_SIZE);
                comp_ptr[0] = range / half(255.0f); // inv_scale
                comp_ptr[1] = zp_val;
                // Zero-fill remaining tokens (lazy init) not required now; read paths will ignore until written.
            }
        };

        auto requantize_and_append_channel = [&](const vector<half, K_HEAD_SIZE>& token_vec,
                                                 uchar* out,
                                                 uint block_base_offset,
                                                 uint token_pos) {
            // token_pos > 0: need previous inv_scale/zp, dequant all previous tokens, add new, recompute params and re-pack.
            #pragma unroll
            for (uint c = 0; c < K_HEAD_SIZE; ++c) {
                uint channel_offset = block_base_offset + c * (ADJUSTED_PAGED_ATTENTION_BLOCK_SIZE);
                half* comp_ptr_old = (half*)(out + channel_offset + PAGED_ATTENTION_BLOCK_SIZE);
                half inv_scale_old = comp_ptr_old[0];
                half zp_old = comp_ptr_old[1];
                vector<half, PAGED_ATTENTION_BLOCK_SIZE> temp_vals; // reuse first token_pos+1 entries
                half max_val = half(-65504.0f);
                half min_val = half(65504.0f);
                // Dequant previous tokens
                for (uint t = 0; t < token_pos; ++t) {
                    uchar q = out[channel_offset + t];
                    half dq = (half(q) - zp_old) * inv_scale_old;
                    temp_vals[t] = dq;
                    max_val = max(dq, max_val);
                    min_val = min(dq, min_val);
                }
                // Append new token
                half new_v = token_vec[c];
                temp_vals[token_pos] = new_v;
                max_val = max(new_v, max_val);
                min_val = min(new_v, min_val);
                half range = max_val - min_val;
                if (range == half(0.0f)) range = half(1.0f / 255.0f);
                half scale_val = half(255.0f) / range;
                half zp_val = (half(0.0f) - min_val) * scale_val;
                half inv_scale = range / half(255.0f);
                // Requantize all existing tokens (token_pos+1 valid)
                for (uint t = 0; t <= token_pos; ++t) {
                    half q_half = temp_vals[t] * scale_val + zp_val;
                    int q_int = (int)cm_rnde_sat<uchar>(q_half);
                    out[channel_offset + t] = (uchar)q_int;
                }
                half* comp_ptr_new = (half*)(out + channel_offset + PAGED_ATTENTION_BLOCK_SIZE);
                comp_ptr_new[0] = inv_scale;
                comp_ptr_new[1] = zp_val;
            }
        };
        #endif // IS_KEY_BY_CHANNEL
    #endif // KV_CACHE_COMPRESSION_PER_TOKEN

    {
        // Compute base offset depending on layout
        uint block_k_base_offset;
        #if KV_CACHE_COMPRESSION_PER_TOKEN
            #ifdef IS_KEY_BY_CHANNEL
                block_k_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * (K_HEAD_SIZE * ADJUSTED_PAGED_ATTENTION_BLOCK_SIZE);
            #else
                block_k_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
            #endif
        #else
            block_k_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
        #endif
        uint key_out_offset = block_k_base_offset + token_start_pos * K_HEAD_SIZE;
        uint key_in_offset = token_idx * key_pitch + head_idx * K_HEAD_SIZE + key_offset;

        vector<half, K_HEAD_SIZE> key_data;
        key_data.format<int>() = cm_ptr_load<int, K_HEAD_SIZE / 2>((int*)key, key_in_offset * (int)sizeof(half));

        #if KV_CACHE_COMPRESSION_PER_TOKEN
            #ifdef IS_KEY_BY_CHANNEL
                if (token_start_pos == 0) {
                    quantize_and_store_channel_block_init(key_data, (uchar*)key_cache, block_k_base_offset);
                } else {
                    requantize_and_append_channel(key_data, (uchar*)key_cache, block_k_base_offset, token_start_pos);
                }
            #else
                quantize_and_store_token(key_data, (uchar*)key_cache, block_k_base_offset, token_start_pos);
            #endif
        #else
            cm_ptr_store<int, K_HEAD_SIZE / 2>((int*)key_cache, key_out_offset * (int)sizeof(half), key_data.format<int>());
        #endif
    }
    {
        uint block_v_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * ADJUSTED_V_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
        uint value_out_offset = block_v_base_offset + token_start_pos * V_HEAD_SIZE;
        uint value_in_offset = token_idx * value_pitch + head_idx * V_HEAD_SIZE + value_offset;

        vector<half, V_HEAD_SIZE> value_data;
        value_data.format<int>() = cm_ptr_load<int, V_HEAD_SIZE / 2>((int*)value, value_in_offset * (int)sizeof(half));
        #if KV_CACHE_COMPRESSION_PER_TOKEN
            // Value still per-token compression (no by-channel path yet)
            quantize_and_store_token(value_data, (uchar*)value_cache, block_v_base_offset, token_start_pos);
        #else
            cm_ptr_store<int, V_HEAD_SIZE / 2>((int*)value_cache, value_out_offset * (int)sizeof(half), value_data.format<int>());
        #endif
    }
}
